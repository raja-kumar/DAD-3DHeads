{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "objc[6269]: Class CaptureDelegate is implemented in both /Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x16450e538) and /Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x28cb48860). One of the two will be used. Which one is undefined.\n",
      "objc[6269]: Class CVWindow is implemented in both /Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x16450e588) and /Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x28bfd8a68). One of the two will be used. Which one is undefined.\n",
      "objc[6269]: Class CVView is implemented in both /Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x16450e5b0) and /Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x28bfd8a90). One of the two will be used. Which one is undefined.\n",
      "objc[6269]: Class CVSlider is implemented in both /Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x16450e5d8) and /Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x28bfd8ab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import os\n",
    "from pytorch_toolbelt.utils import read_rgb_image\n",
    "from predictor import FaceMeshPredictor\n",
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "from model_training.utils import load_indices_from_npy\n",
    "from utils import get_relative_path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this functions take an image as input and returns it's 68 landmark coordinates using dadnet network. \n",
    "# For more number of landmarks see run_dad_net_more_points function below\n",
    "def run_dad_net(image_path):\n",
    "    image = read_rgb_image(image_path)\n",
    "    predictor = FaceMeshPredictor.dad_3dnet()\n",
    "    predictions = predictor(image)\n",
    "    print(type(predictions))\n",
    "    coordinates = predictions['points']\n",
    "    x_pred,y_pred = coordinates[:,0], coordinates[:,1]\n",
    "    return x_pred, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function uses mediapipe api to detect face and saves the images after cropping only the bounding box\n",
    "def mediapipe_face_detection(data_path, output_path):\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    #IMAGE_FILES = [\"../Dataset/H3DS_sample/sample2/original/0/img_0009.jpg\"]\n",
    "    IMAGE_FILES = []\n",
    "\n",
    "    files = os.listdir(data_path)\n",
    "\n",
    "    for file in files:\n",
    "        curr_image = os.path.join(data_path, file)\n",
    "        if (file.split('.')[-1] == 'jpg'):\n",
    "            IMAGE_FILES.append(curr_image)\n",
    "\n",
    "    #print(len(IMAGE_FILES))\n",
    "    bboxes = {}\n",
    "\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        for idx, file in enumerate(IMAGE_FILES):\n",
    "            #print(\"sancjnajcdd\")\n",
    "            image = cv2.imread(file)\n",
    "            # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "            results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            if not results.detections:\n",
    "                continue\n",
    "            annotated_image = image.copy()\n",
    "\n",
    "            for detection in results.detections:\n",
    "                \"\"\"print('Nose tip:')\n",
    "                print(mp_face_detection.get_key_point(\n",
    "                    detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\"\"\"\n",
    "                mp_drawing.draw_detection(annotated_image, detection)\n",
    "            curr_bbox = results.detections[0].location_data.relative_bounding_box\n",
    "\n",
    "            if (curr_bbox.xmin < 0 or curr_bbox.ymin < 0):\n",
    "                continue\n",
    "            results.detections[0].location_data.relative_bounding_box\n",
    "            output_image = os.path.join(output_path, os.path.basename(file))\n",
    "            filename = os.path.basename(file)\n",
    "            # get the bounding box parameters which is returned by media pipe api\n",
    "            bboxes[filename] = results.detections[0].location_data.relative_bounding_box\n",
    "            #print(output_image)\n",
    "            #print(bboxes[filename])\n",
    "            xmin = int(bboxes[filename].xmin*image.shape[1]) ## finds the xmin of bounding box\n",
    "            ymin = int(bboxes[filename].ymin*image.shape[0]) ## fidn the ymin of bounding box\n",
    "            w,h = int(bboxes[filename].width*image.shape[1]), int(bboxes[filename].height*image.shape[0]) # find the width and height of bounding box\n",
    "            detected_portion = image[ymin:ymin+h, xmin:xmin+w] # find the detected region in the image\n",
    "            #print(detected_portion)\n",
    "            #print(output_image)\n",
    "            cv2.imwrite(output_image, detected_portion) #save the detected portion\n",
    "        \n",
    "        return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes as original image with a face in it and corresponding detected face images (generated using above mediapipe function).\n",
    "# it runs the DadNet on detected face and find the coordiantes on original image.\n",
    "\n",
    "def run_dadnet_on_large_images(original_images, data_path, output_path, bboxes):\n",
    "    #data_path is the path to the detected faces images (not the full image)\n",
    "    IMAGE_FILES = []\n",
    "\n",
    "    files = os.listdir(data_path)\n",
    "\n",
    "    for file in files:\n",
    "        curr_image = os.path.join(data_path, file)\n",
    "        IMAGE_FILES.append(curr_image)\n",
    "\n",
    "    for idx, file in enumerate(IMAGE_FILES):\n",
    "        pred_x,pred_y = run_dad_net(file)\n",
    "\n",
    "        filename = os.path.basename(file)\n",
    "        original_image = os.path.join(original_images, filename)\n",
    "        image = cv2.imread(original_image)\n",
    "        xmin = int(bboxes[filename].xmin*image.shape[1]) # find the xmin of detected face wrt to original image\n",
    "        ymin = int(bboxes[filename].ymin*image.shape[0]) # find the ymin of the detected face wrt to original image\n",
    "        pred_x_new, pred_y_new = xmin + pred_x, ymin + pred_y #the coordiantes of landmarks on the original image\n",
    "\n",
    "        output_file = os.path.join(output_path, filename) \n",
    "\n",
    "        for idx,point in enumerate(zip(pred_x_new, pred_y_new)):\n",
    "            image = cv2.circle(image, point, radius=6, color=(0, 0, 255), thickness=-1) #save the original image with landmark on it\n",
    "        \n",
    "        cv2.imwrite(output_file, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dad_net_more_points(image_path, num_points):\n",
    "\n",
    "    # keypoint dir contain the index of each keypoints that we want to consider for our landmarks. \n",
    "    # Only these points are extracted from the projected list of indices\n",
    "\n",
    "    if (num_points == 445):\n",
    "        keypoint_dir = \"model_training/model/static/face_keypoints/keypoints_445/\"\n",
    "    elif(num_points == 191):\n",
    "        keypoint_dir = \"model_training/model/static/face_keypoints/keypoints_191/\"\n",
    "    else:\n",
    "        ValueError(\"Invalid keypoints subset provided.\\n\"\n",
    "                   \"Available options are: 191, 445\")\n",
    "    \n",
    "    image = read_rgb_image(image_path)\n",
    "    predictor = FaceMeshPredictor.dad_3dnet()\n",
    "    predictions = predictor(image)\n",
    "\n",
    "    subset_dir = []\n",
    "    for file in os.listdir(keypoint_dir):\n",
    "        subset_dir.append(os.path.join(keypoint_dir, file))\n",
    "    projected_vertices = predictions[\"projected_vertices\"].squeeze().numpy().astype(int) # projected vertices (total of 5023)\n",
    "    #print(projected_vertices.shape)\n",
    "    points = []\n",
    "#print(subset_dir)\n",
    "    for subs_file in subset_dir:\n",
    "        #subs_file_path = os.path.join(subset_dir, subs_file)\n",
    "        #print(len(np.take(projected_vertices, load_indices_from_npy(subs_file), axis=0)))\n",
    "        points.extend(np.take(projected_vertices, load_indices_from_npy(subs_file), axis=0))\n",
    "    \n",
    "    pred_x,pred_y = [], []\n",
    "    for point in points:\n",
    "        pred_x.append(point[0])\n",
    "        pred_y.append(point[1])\n",
    "    \n",
    "    ### this is to test all the 5023 points. comment out below two lines\n",
    "\n",
    "    #pred_x = projected_vertices[:,0]\n",
    "    #pred_y = projected_vertices[:,1]\n",
    "    \n",
    "    return np.array(pred_x), np.array(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dadnet_on_large_images_more_points(original_images, data_path, output_path, bboxes, n_points):\n",
    "    #data_path is the path to the detected faces images (not the full image)\n",
    "    IMAGE_FILES = []\n",
    "\n",
    "    files = os.listdir(data_path)\n",
    "\n",
    "    for file in files:\n",
    "        curr_image = os.path.join(data_path, file)\n",
    "        IMAGE_FILES.append(curr_image)\n",
    "\n",
    "    for idx, file in enumerate(IMAGE_FILES):\n",
    "        pred_x,pred_y = run_dad_net_more_points(file,n_points)\n",
    "\n",
    "        filename = os.path.basename(file)\n",
    "        original_image = os.path.join(original_images, filename)\n",
    "        image = cv2.imread(original_image)\n",
    "        xmin = int(bboxes[filename].xmin*image.shape[1])\n",
    "        ymin = int(bboxes[filename].ymin*image.shape[0])\n",
    "        pred_x_new, pred_y_new = xmin + pred_x, ymin + pred_y\n",
    "\n",
    "        output_file = os.path.join(output_path, filename)\n",
    "\n",
    "        for idx,point in enumerate(zip(pred_x_new, pred_y_new)):\n",
    "            image = cv2.circle(image, point, radius=3, color=(0, 0, 255), thickness=-1)\n",
    "        \n",
    "        cv2.imwrite(output_file, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first find the bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "facescape_sample5 = \"../Dataset/facescape_sample/sample5/512/1/\"\n",
    "detected_path = \"../Dataset/facescape_sample/sample5/512_detected/1\"\n",
    "bounding_boxes_facescape = mediapipe_face_detection(facescape_sample5,detected_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### run for 68 landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return forward_call(*input, **kwargs)\n",
      "/Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/torch/nn/modules/module.py:1051: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./outputs/facescape_sample/sample3/detect_mp_dad_68/\"\n",
    "run_dadnet_on_large_images(facescape_sample2,detected_path, output_path, bounding_boxes_facescape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### run for 191/445 landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./outputs/facescape_sample/sample3/detect_mp_dad_191/\"\n",
    "run_dadnet_on_large_images_more_points(facescape_sample2,detected_path, output_path, bounding_boxes_facescape,191)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### landmarks for custom index list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dad_net_custom_points(image_path, indices_file):\n",
    "\n",
    "    # pass a numpy indices to get those points \n",
    "    \n",
    "    image = read_rgb_image(image_path)\n",
    "    predictor = FaceMeshPredictor.dad_3dnet()\n",
    "    predictions = predictor(image)\n",
    "    projected_vertices = predictions[\"projected_vertices\"].squeeze().numpy().astype(int) # projected vertices (total of 5023)\n",
    "    #print(projected_vertices.shape)\n",
    "    indices = np.load(indices_file)\n",
    "    points = []\n",
    "    points.extend(np.take(projected_vertices, indices, axis=0))\n",
    "    \n",
    "    pred_x,pred_y = [], []\n",
    "    for point in points:\n",
    "        pred_x.append(point[0])\n",
    "        pred_y.append(point[1])\n",
    "    \n",
    "    return np.array(pred_x), np.array(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dadnet_on_large_images_custom_points(original_images, data_path, output_path, bboxes, indices_file):\n",
    "    #data_path is the path to the detected faces images (not the full image)\n",
    "    IMAGE_FILES = []\n",
    "\n",
    "    files = os.listdir(data_path)\n",
    "\n",
    "    for file in files:\n",
    "        curr_image = os.path.join(data_path, file)\n",
    "        IMAGE_FILES.append(curr_image)\n",
    "\n",
    "    for idx, file in enumerate(IMAGE_FILES):\n",
    "        pred_x,pred_y = run_dad_net_custom_points(file,indices_file)\n",
    "\n",
    "        filename = os.path.basename(file)\n",
    "        original_image = os.path.join(original_images, filename)\n",
    "        image = cv2.imread(original_image)\n",
    "        xmin = int(bboxes[filename].xmin*image.shape[1])\n",
    "        ymin = int(bboxes[filename].ymin*image.shape[0])\n",
    "        pred_x_new, pred_y_new = xmin + pred_x, ymin + pred_y\n",
    "\n",
    "        output_file = os.path.join(output_path, filename)\n",
    "        lmk_file = os.path.join(output_path,os.path.basename(filename).split(\".\")[0] + \".npy\")\n",
    "        lmks = []\n",
    "        for idx,point in enumerate(zip(pred_x_new, pred_y_new)):\n",
    "            lmks.append(point)\n",
    "            image = cv2.circle(image, point, radius=3, color=(0, 0, 255), thickness=-1)\n",
    "        \n",
    "        lmks = np.array(lmks)\n",
    "        #print(lmk_file)\n",
    "        np.save(lmk_file, lmks)\n",
    "        #cv2.imwrite(output_file, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return forward_call(*input, **kwargs)\n",
      "/Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/torch/nn/modules/module.py:1051: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./outputs/facescape_sample/sample5/1/\"\n",
    "indices_file = \"./model_training/model/static/face_keypoints/ids1.npy\"\n",
    "run_dadnet_on_large_images_custom_points(facescape_sample5,detected_path, output_path, bounding_boxes_facescape,indices_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.load(\"./outputs/facescape_sample/sample3/detect_mp_dad_1797/12.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run mp_dad on multiple folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "66\n",
      "18\n",
      "59\n",
      "341\n",
      "57\n",
      "9\n",
      "59\n",
      "11\n",
      "68\n",
      "7\n",
      "57\n",
      "16\n",
      "57\n",
      "6\n",
      "56\n",
      "17\n",
      "58\n",
      "1\n",
      "55\n",
      "340\n",
      "55\n",
      "19\n",
      "57\n",
      "121\n",
      "56\n",
      "212\n",
      "57\n",
      "342\n",
      "57\n",
      "4\n",
      "54\n",
      "15\n",
      "68\n",
      "3\n",
      "56\n",
      "122\n",
      "59\n",
      "2\n",
      "57\n",
      "13\n",
      "57\n",
      "5\n",
      "58\n",
      "14\n",
      "68\n",
      "344\n",
      "56\n",
      "343\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "full_dataset = \"../Dataset/facescape_sample/sample5/512/\"\n",
    "detected_path = \"../Dataset/facescape_sample/sample5/512_detected/\"\n",
    "bounding_boxes_facescape = {}\n",
    "for path in os.listdir(full_dataset):\n",
    "    #print(path)\n",
    "    curr_facescape_sample = os.path.join(full_dataset, path)\n",
    "    curr_detected_path = os.path.join(detected_path, path)\n",
    "    if not os.path.exists(curr_detected_path):\n",
    "        os.makedirs(curr_detected_path)\n",
    "    bounding_boxes_facescape[path] = mediapipe_face_detection(curr_facescape_sample,curr_detected_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./outputs/facescape_sample/sample5/\"\n",
    "indices_file = \"./model_training/model/static/face_keypoints/ids1.npy\"\n",
    "for path in os.listdir(detected_path):\n",
    "    curr_facescape_sample = os.path.join(full_dataset, path)\n",
    "    curr_detected_path = os.path.join(detected_path, path)\n",
    "    curr_output_path = os.path.join(output_path, path)\n",
    "    if not os.path.exists(curr_output_path):\n",
    "        os.makedirs(curr_output_path)\n",
    "    run_dadnet_on_large_images_custom_points(curr_facescape_sample,curr_detected_path, curr_output_path, bounding_boxes_facescape[path],indices_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 2)\n"
     ]
    }
   ],
   "source": [
    "landmarks = np.load(\"./outputs/facescape_sample/sample5/1/1.npy\")\n",
    "print(landmarks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(\"../Dataset/facescape_sample/sample5/512/1/1.jpg\")\n",
    "for point in landmarks:\n",
    "    image = cv2.circle(image, point, radius=3, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "cv2.imwrite('test1.jpg', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rajakumar/Desktop/MS/Fall-2022/research/2D_Landmarks/DAD-3DHeads\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run for stirling and H3DS test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "68\n",
      "68\n",
      "69\n",
      "70\n",
      "68\n",
      "69\n",
      "70\n",
      "68\n",
      "69\n",
      "65\n",
      "68\n",
      "69\n",
      "68\n",
      "66\n",
      "69\n",
      "68\n",
      "73\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "full_dataset ='../Dataset/H3DS_sample/test_set/H3DS/'\n",
    "detected_path = \"../Dataset/H3DS_sample/test_set/H3DS_detected/\"\n",
    "bounding_boxes_h3ds = {}\n",
    "for path in os.listdir(full_dataset):\n",
    "    #print(path)\n",
    "    curr_hds_sample = os.path.join(full_dataset, path)\n",
    "    curr_detected_path = os.path.join(detected_path, path)\n",
    "    if not os.path.exists(curr_detected_path):\n",
    "        os.makedirs(curr_detected_path)\n",
    "    bounding_boxes_h3ds[path] = mediapipe_face_detection(curr_hds_sample,curr_detected_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return forward_call(*input, **kwargs)\n",
      "/Users/rajakumar/miniforge3/envs/dad_3d/lib/python3.8/site-packages/torch/nn/modules/module.py:1051: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./outputs/H3DS/test_set/\"\n",
    "indices_file = \"./model_training/model/static/face_keypoints/ids1.npy\"\n",
    "for path in os.listdir(detected_path):\n",
    "    curr_hds_sample = os.path.join(full_dataset, path)\n",
    "    curr_detected_path = os.path.join(detected_path, path)\n",
    "    curr_output_path = os.path.join(output_path, path)\n",
    "    if not os.path.exists(curr_output_path):\n",
    "        os.makedirs(curr_output_path)\n",
    "    run_dadnet_on_large_images_custom_points(curr_hds_sample,curr_detected_path, curr_output_path, bounding_boxes_h3ds[path],indices_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmk_path = \"./outputs/H3DS/test_set/0cd3f3c0bc34a287/\"\n",
    "original_data_path = \"../Dataset/H3DS_sample/test_set/H3DS/0cd3f3c0bc34a287/\"\n",
    "\n",
    "for data in os.listdir(lmk_path):\n",
    "\n",
    "    filename = data.split(\".\")[0]\n",
    "    curr_lmk = os.path.join(lmk_path, filename + \".npy\")\n",
    "    curr_img = os.path.join(original_data_path, filename+'.jpg')\n",
    "    output_path = os.path.join(lmk_path, filename + '.jpg')\n",
    "    #print(output_path)\n",
    "    landmarks = np.load(curr_lmk)\n",
    "    #print(landmarks.shape)\n",
    "    image = cv2.imread(curr_img)\n",
    "    for point in landmarks:\n",
    "        image = cv2.circle(image, point, radius=3, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset ='../Dataset/stirling/stirling_test/'\n",
    "detected_path = \"../Dataset/stirling/test_detected/\"\n",
    "bounding_boxes_h3ds = {}\n",
    "for path in os.listdir(full_dataset):\n",
    "    #print(path)\n",
    "    curr_hds_sample = os.path.join(full_dataset, path)\n",
    "    curr_detected_path = os.path.join(detected_path, path)\n",
    "    if not os.path.exists(curr_detected_path):\n",
    "        os.makedirs(curr_detected_path)\n",
    "    bounding_boxes_h3ds[path] = mediapipe_face_detection(curr_hds_sample,curr_detected_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./outputs/stirling/test_set/\"\n",
    "indices_file = \"./model_training/model/static/face_keypoints/ids1.npy\"\n",
    "for path in os.listdir(detected_path):\n",
    "    curr_hds_sample = os.path.join(full_dataset, path)\n",
    "    curr_detected_path = os.path.join(detected_path, path)\n",
    "    curr_output_path = os.path.join(output_path, path)\n",
    "    if not os.path.exists(curr_output_path):\n",
    "        os.makedirs(curr_output_path)\n",
    "    run_dadnet_on_large_images_custom_points(curr_hds_sample,curr_detected_path, curr_output_path, bounding_boxes_h3ds[path],indices_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmk_path = \"./outputs/stirling/test_set/F1003/\"\n",
    "original_data_path = \"../Dataset/stirling/stirling_test/F1003/\"\n",
    "\n",
    "for data in os.listdir(lmk_path):\n",
    "\n",
    "    filename = data.split(\".\")[0]\n",
    "    curr_lmk = os.path.join(lmk_path, filename + \".npy\")\n",
    "    curr_img = os.path.join(original_data_path, filename+'.jpg')\n",
    "    output_path = os.path.join(lmk_path, filename + '.jpg')\n",
    "    #print(output_path)\n",
    "    landmarks = np.load(curr_lmk)\n",
    "    #print(landmarks.shape)\n",
    "    image = cv2.imread(curr_img)\n",
    "    for point in landmarks:\n",
    "        image = cv2.circle(image, point, radius=3, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dad_3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5796df81dbfa9d73d5f5430114a8c6003918df0f1ffc6a75c58a987af26b37fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
